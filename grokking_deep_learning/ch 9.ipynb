{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2886e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions must be  (1) continuous and (2) monotonic (never changing direction)\n",
    "# for ex a parabala (y = x * x) would be bad b/c multiple values of x return the same y \n",
    "# so there would be multiple \"correct\" answers for weights. this is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should also be (3) non-linear i.e they squiggle or turn (like relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if function was linear, it would just scale weighted averages coming in - this linear activation\n",
    "# doesn't allow one weight to affect how correlated the neuron is to the other weights. we need\n",
    "# selective correlation - given a neuron with an activation function we want one incoming signal to be \n",
    "# able to increase or decrease how correlated the neuron is to all the other incoming signals. all\n",
    "# curved lines do this (to varying degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) activation function and its derivative should be efficiently computable since we'll be calling\n",
    "# the function a ton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard hidden layer activation functions\n",
    "#    sigmoid - let's you interpret the output of any individual neuron as a probability ((0, 1) output)\n",
    "#    tanh (better than sigmoid for hidden layers) - basically a sigmoid except it's between ((-1, 1) output)\n",
    "#        this means it can also throw in some negative correlation. not as useful for output layers unless\n",
    "#        our output should be in the range (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard output layer activation functions\n",
    "# main focus is ensuring the output nonlinearity can predict the right answers. signmoid or tanh obvi wouldn't\n",
    "# be appropriate for something like temperature\n",
    "\n",
    "#    raw values (no activation function) - appropriate if we want to train a NN to transform 1 matrix into another\n",
    "#        where output is something other than a probability (like avg temperature in Colorado or something)\n",
    "#    sigmoid\n",
    "#    softmax (most common) - the more likely it's one label, the less likely it's any of the other labels\n",
    "#        like for MNIST digit classifier - le'ts say we're looking at a 9\n",
    "#        labels   0   1   2   3   4   5   6   7   8   9\n",
    "#        raw      0   0   0   0   0   0   0   0   0   0\n",
    "#        sigmoid .5  .5  .5  .5  .5  .5  .5  .5  .5  .99 (eww)\n",
    "#        softmax  0   0   0   0   0   0   0   0   0   1  (always sums to 1)\n",
    "\n",
    "#        even though it looks sigmoid predicts nearly perfectly, it will backprop a ton of error b/c for sigmoid\n",
    "#        to reach 0 error, it doesn't just have to predict the highest positive number for the true output, it\n",
    "#        has to predict 0 everywhere else. where softmax asks \"which digit seems like the best fit for this input\"\n",
    "#        sigmoid says \"it's only digit 9 and doesn't have anything in common with the other MNIST digits\"\n",
    "\n",
    "#        we want output that won't penalize labels that are similar, instead we want it to pay attention to all\n",
    "#        the info that can be indicative of any potential input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f000240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation in forward prop pretty straight forward (just pass the layer_0 * weights_0_1 into the activation function)\n",
    "# backprop activation more nuanced. to generate layer_1 delta, we have to multiply the backpropagated delta from\n",
    "# layer 2 (layer_2_delta.dot(weights_1_2.T)) by the slope (derivative) at the point predicited in forward prop.\n",
    "# for pos numbers delta slope will be 1 and neg numbers will be 0 for relu\n",
    "\n",
    "# what about for other activation functions? for sigmoid, slope get's steeper for values approaching 0 from either direction\n",
    "# and slope approaches 0 as we move towards higher pos and lower neg values, so delta will matter much more for values\n",
    "# closer to 0 and values further from 0 get multipled by a slope of closer to 0 so delta closer to 0. this also\n",
    "# creates a notion of stickiness - weights that have previously been updated a lot in one direction confidently \n",
    "# these nonlinearities help make it harder for occasional erroneous training examples to corrupt intelligence\n",
    "# that has been reinforced many times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting output to slope (derivative)\n",
    "\n",
    "# most nonlinearities (all the popular ones) use a method for computing derivative that's a lil strange.\n",
    "# instead of computing derivative at a certain point on the curve the normal way, most great activation functions\n",
    "# have a means by which the output of the layer (at forward prop) can be used to compute the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607015f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try using tanh for hidden layer activation and softmax for output-layer activation on MNIST now.\n",
    "# we'll need to initialize our weights differently for tanh. relu is okay with (-.1, 0.1) but tanh\n",
    "# likes narrower values like (-0.01, 0.01) so we'll just scale down our weights from the previous example.\n",
    "\n",
    "# BUT because we're using softmax now, error should be calculated with a different error function... not\n",
    "# ready for that yet so we'll just remove lines to compute error fer now.\n",
    "\n",
    "# also need to adjust alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710bd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations, hidden_size = (2, 300, 100) # alpha diff\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "weights_0_1 = 0.02*np.random.random((pixels_per_image, hidden_size)) -0.01 # scale to narrower range for tanh activation\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1 #same range as before is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc4a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 test-acc:0.394 train-acc:0.156\n",
      "I:10 test-acc:0.6867 train-acc:0.723\n",
      "I:20 test-acc:0.7025 train-acc:0.732\n",
      "I:30 test-acc:0.734 train-acc:0.763\n",
      "I:40 test-acc:0.7663 train-acc:0.794\n",
      "I:50 test-acc:0.7913 train-acc:0.819\n",
      "I:60 test-acc:0.8102 train-acc:0.849\n",
      "I:70 test-acc:0.8228 train-acc:0.864\n",
      "I:80 test-acc:0.831 train-acc:0.867\n",
      "I:90 test-acc:0.8364 train-acc:0.885\n",
      "I:100 test-acc:0.8407 train-acc:0.883\n",
      "I:110 test-acc:0.845 train-acc:0.891\n",
      "I:120 test-acc:0.8481 train-acc:0.901\n",
      "I:130 test-acc:0.8505 train-acc:0.901\n",
      "I:140 test-acc:0.8526 train-acc:0.905\n",
      "I:150 test-acc:0.8555 train-acc:0.914\n",
      "I:160 test-acc:0.8577 train-acc:0.925\n",
      "I:170 test-acc:0.8596 train-acc:0.918\n",
      "I:180 test-acc:0.8619 train-acc:0.933\n",
      "I:190 test-acc:0.863 train-acc:0.933\n",
      "I:200 test-acc:0.8642 train-acc:0.926\n",
      "I:210 test-acc:0.8653 train-acc:0.931\n",
      "I:220 test-acc:0.8668 train-acc:0.93\n",
      "I:230 test-acc:0.8672 train-acc:0.937\n",
      "I:240 test-acc:0.8681 train-acc:0.938\n",
      "I:250 test-acc:0.8687 train-acc:0.937\n",
      "I:260 test-acc:0.8684 train-acc:0.945\n",
      "I:270 test-acc:0.8703 train-acc:0.951\n",
      "I:280 test-acc:0.8699 train-acc:0.949\n",
      "I:290 test-acc:0.8701 train-acc:0.94"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end = ((i * batch_size), ((i+1)*batch_size)) # train in batches\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) # randomly turn off nodes\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2)) #softmax baybee\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == \\\n",
    "                               np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "        \n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1) # use layer 1 output to calc deriv\n",
    "        layer_1_delta *= dropout_mask\n",
    "            \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)  # no softmax?\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "            \n",
    "    if(j % 10 == 0):\n",
    "        sys.stdout.write(\"\\n\"+ \"I:\" + str(j) + \\\n",
    "                         \" test-acc:\" + str(test_correct_cnt/float(len(test_images)))+\\\n",
    "                         \" train-acc:\" + str(correct_cnt/float(len(images))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aad2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grokking",
   "language": "python",
   "name": "grokking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
