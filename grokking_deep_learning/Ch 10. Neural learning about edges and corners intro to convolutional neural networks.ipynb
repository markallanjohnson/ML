{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting often caused by having more params than necessary to learn a specific dataset.\n",
    "# in our case, we have so many params that it can memeorize every detail in training instead\n",
    "# of learning high-level abstractions. when nns have many params but not very many training \n",
    "# examples, overfitting diffi to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea807d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used regularization previously to help with overfitting, but we have more tools avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting concerned with the ratio between # of weights in the model and the number of \n",
    "# datapoints it has to learn. so there's a better method to combat overfitting.\n",
    "# we use \"structure\" (loosely defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure is when we selectively choose to reuse weights for multi purposes b/c we\n",
    "# believe the same pattern needs to be detected in multiple places (like a 2 vs a 3 in MNIST\n",
    "# 2 and 3 have certain things in common so we can reuse weights for both). this will reduce\n",
    "# weight-to-data ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to be clever about it though. if we just removed params the model would be less expressive.\n",
    "# we want equally expressive but more robust to overfitting. (model will be smaller though b/c\n",
    "# fewer params to store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"convolutional layer\" - most widely used structure in nns\n",
    "#    lots of very small linear layers are reused in every position, instead of a single beeg one.\n",
    "#    our large dense layer had a connection from efvery input to every output, but with our\n",
    "#    convolutional layer, we instead have lots of very small linear layers, usually with fewer\n",
    "#    than 25 inputs and a single output, which we use in every input position.\n",
    "#    each mini layer is called a \"convolutional kernel\" - but it's basically a baby linear layer with \n",
    "#    small # of inputs and a single output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a 3x3 convolutional kernel, it would predict in its current location (top left) and move 1 pixel\n",
    "# at a time to the right, then predict again, and so on. once its scanned across the image, it will \n",
    "# move down a single pixel and scan back to the left, and then repeating until it has made a pred\n",
    "# in every possible positin within the image. result will be a smaller square of kernel predictions, \n",
    "# which are used in input to the next layer. result will be a 6x6 prediciton matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we used 4 diff 3x3 convo kernels looking at the same spot of an 8x8 image of a 2\n",
    "# each kernel results in a 6x6 pred matrix. so we have 4 6x6 pred matrices.\n",
    "# (1) we can sum them elementwise (sum pooling)\n",
    "# (2) take the mean elementwise (mean pooling)\n",
    "# (3) compute the elementwise max value (max pooling)\n",
    "\n",
    "# max pooling is most popular. for each position, look into each of the 4 kernel's outputs\n",
    "# find the max, and copy it into the final 6x6 matrix. this 6x6 matrix will then forward \n",
    "# propogate into the next layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this technique allows each kernel to learn a particular pattern and then search for the \n",
    "# existence of that pattern somewhere in the image.\n",
    "\n",
    "# a single small set of weights can train over a much larger set of training examples b/c\n",
    "# even though the dataset hasn't changed, each mini-kernel is forward propagated multiple times\n",
    "# on multiple segments of data, thus changing the ratio of weights to datapoints on which those \n",
    "# weights are being trained. this will drastically reduce the nns ability to overfit training data.\n",
    "# / increase its ability to generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement in NumPy\n",
    "\n",
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "    \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "\n",
    "# 16 3x3 -> 16 6x6\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "\n",
    "hidden_size = ((input_rows - kernel_rows) * (input_cols - kernel_cols)) * num_kernels\n",
    "\n",
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    '''select a subregion in a batch of images.'''\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1, 1, row_to-row_from, col_to-col_from)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8192144",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols, num_kernels))-0.01 #kernals.shape == (9, 16)\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels))-0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc06bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i:0 test-acc:0.0288 train-acc:0.055\n",
      "i:1 test-acc:0.0273 train-acc:0.037\n",
      "i:2 test-acc:0.028 train-acc:0.037\n",
      "i:3 test-acc:0.0292 train-acc:0.04\n",
      "i:4 test-acc:0.0339 train-acc:0.046\n",
      "i:5 test-acc:0.0478 train-acc:0.068\n",
      "i:6 test-acc:0.076 train-acc:0.083\n",
      "i:7 test-acc:0.1316 train-acc:0.096\n",
      "i:8 test-acc:0.2137 train-acc:0.127\n",
      "i:9 test-acc:0.2941 train-acc:0.148\n",
      "i:10 test-acc:0.3563 train-acc:0.181\n",
      "i:11 test-acc:0.4023 train-acc:0.209\n",
      "i:12 test-acc:0.4358 train-acc:0.238\n",
      "i:13 test-acc:0.4473 train-acc:0.286\n",
      "i:14 test-acc:0.4389 train-acc:0.274\n",
      "i:15 test-acc:0.3951 train-acc:0.257\n",
      "i:16 test-acc:0.2222 train-acc:0.243\n",
      "i:17 test-acc:0.0613 train-acc:0.112\n",
      "i:18 test-acc:0.0266 train-acc:0.035\n",
      "i:19 test-acc:0.0127 train-acc:0.026\n",
      "i:20 test-acc:0.0133 train-acc:0.022\n",
      "i:21 test-acc:0.0185 train-acc:0.038\n",
      "i:22 test-acc:0.0363 train-acc:0.038\n",
      "i:23 test-acc:0.0928 train-acc:0.067\n",
      "i:24 test-acc:0.1994 train-acc:0.081\n",
      "i:25 test-acc:0.3086 train-acc:0.154\n",
      "i:26 test-acc:0.4276 train-acc:0.204\n",
      "i:27 test-acc:0.5323 train-acc:0.256\n",
      "i:28 test-acc:0.5919 train-acc:0.305\n",
      "i:29 test-acc:0.6324 train-acc:0.341\n",
      "i:30 test-acc:0.6608 train-acc:0.426\n",
      "i:31 test-acc:0.6815 train-acc:0.439\n",
      "i:32 test-acc:0.7048 train-acc:0.462\n",
      "i:33 test-acc:0.7171 train-acc:0.484\n",
      "i:34 test-acc:0.7313 train-acc:0.505\n",
      "i:35 test-acc:0.7355 train-acc:0.53\n",
      "i:36 test-acc:0.7417 train-acc:0.548\n",
      "i:37 test-acc:0.747 train-acc:0.534\n",
      "i:38 test-acc:0.7491 train-acc:0.55\n",
      "i:39 test-acc:0.7459 train-acc:0.562\n",
      "i:40 test-acc:0.7352 train-acc:0.54\n",
      "i:41 test-acc:0.7082 train-acc:0.496\n",
      "i:42 test-acc:0.6487 train-acc:0.456\n",
      "i:43 test-acc:0.5209 train-acc:0.353\n",
      "i:44 test-acc:0.3305 train-acc:0.234\n",
      "i:45 test-acc:0.2052 train-acc:0.174\n",
      "i:46 test-acc:0.2149 train-acc:0.136\n",
      "i:47 test-acc:0.2679 train-acc:0.171\n",
      "i:48 test-acc:0.3237 train-acc:0.172\n",
      "i:49 test-acc:0.3581 train-acc:0.186\n",
      "i:50 test-acc:0.4202 train-acc:0.21\n",
      "i:51 test-acc:0.5165 train-acc:0.223\n",
      "i:52 test-acc:0.6007 train-acc:0.262\n",
      "i:53 test-acc:0.6476 train-acc:0.308\n",
      "i:54 test-acc:0.676 train-acc:0.363\n",
      "i:55 test-acc:0.696 train-acc:0.402\n",
      "i:56 test-acc:0.7077 train-acc:0.434\n",
      "i:57 test-acc:0.7204 train-acc:0.441\n",
      "i:58 test-acc:0.7303 train-acc:0.475\n",
      "i:59 test-acc:0.7359 train-acc:0.475\n",
      "i:60 test-acc:0.7401 train-acc:0.525\n",
      "i:61 test-acc:0.7493 train-acc:0.517\n",
      "i:62 test-acc:0.7533 train-acc:0.517\n",
      "i:63 test-acc:0.7606 train-acc:0.538\n",
      "i:64 test-acc:0.7644 train-acc:0.554\n",
      "i:65 test-acc:0.7724 train-acc:0.57\n",
      "i:66 test-acc:0.7788 train-acc:0.586\n",
      "i:67 test-acc:0.7855 train-acc:0.595\n",
      "i:68 test-acc:0.7853 train-acc:0.591\n",
      "i:69 test-acc:0.7925 train-acc:0.605\n",
      "i:70 test-acc:0.7973 train-acc:0.64\n",
      "i:71 test-acc:0.8013 train-acc:0.621\n",
      "i:72 test-acc:0.8029 train-acc:0.626\n",
      "i:73 test-acc:0.8092 train-acc:0.631\n",
      "i:74 test-acc:0.8099 train-acc:0.638\n",
      "i:75 test-acc:0.8156 train-acc:0.661\n",
      "i:76 test-acc:0.8156 train-acc:0.639\n",
      "i:77 test-acc:0.8184 train-acc:0.65\n",
      "i:78 test-acc:0.8216 train-acc:0.67\n",
      "i:79 test-acc:0.8246 train-acc:0.675\n",
      "i:80 test-acc:0.8237 train-acc:0.666\n",
      "i:81 test-acc:0.8273 train-acc:0.673\n",
      "i:82 test-acc:0.8273 train-acc:0.704\n",
      "i:83 test-acc:0.8314 train-acc:0.674\n",
      "i:84 test-acc:0.8292 train-acc:0.686\n",
      "i:85 test-acc:0.8335 train-acc:0.699\n",
      "i:86 test-acc:0.8359 train-acc:0.694\n",
      "i:87 test-acc:0.8375 train-acc:0.704\n",
      "i:88 test-acc:0.8373 train-acc:0.697\n",
      "i:89 test-acc:0.8398 train-acc:0.704\n",
      "i:90 test-acc:0.8393 train-acc:0.687\n",
      "i:91 test-acc:0.8436 train-acc:0.705\n",
      "i:92 test-acc:0.8437 train-acc:0.711\n",
      "i:93 test-acc:0.8446 train-acc:0.721\n",
      "i:94 test-acc:0.845 train-acc:0.719\n",
      "i:95 test-acc:0.8469 train-acc:0.724\n",
      "i:96 test-acc:0.8476 train-acc:0.726\n",
      "i:97 test-acc:0.848 train-acc:0.718\n",
      "i:98 test-acc:0.8496 train-acc:0.719\n",
      "i:99 test-acc:0.85 train-acc:0.73\n",
      "i:100 test-acc:0.8511 train-acc:0.737\n",
      "i:101 test-acc:0.8503 train-acc:0.73\n",
      "i:102 test-acc:0.8504 train-acc:0.717\n",
      "i:103 test-acc:0.8528 train-acc:0.74\n",
      "i:104 test-acc:0.8532 train-acc:0.733\n",
      "i:105 test-acc:0.8537 train-acc:0.73\n",
      "i:106 test-acc:0.8568 train-acc:0.721\n",
      "i:107 test-acc:0.857 train-acc:0.75\n",
      "i:108 test-acc:0.8558 train-acc:0.731\n",
      "i:109 test-acc:0.8578 train-acc:0.744\n",
      "i:110 test-acc:0.8588 train-acc:0.754\n",
      "i:111 test-acc:0.8579 train-acc:0.732\n",
      "i:112 test-acc:0.8582 train-acc:0.747\n",
      "i:113 test-acc:0.8593 train-acc:0.747\n",
      "i:114 test-acc:0.8598 train-acc:0.751\n",
      "i:115 test-acc:0.8603 train-acc:0.74\n",
      "i:116 test-acc:0.86 train-acc:0.753\n",
      "i:117 test-acc:0.8588 train-acc:0.746\n",
      "i:118 test-acc:0.861 train-acc:0.741\n",
      "i:119 test-acc:0.8616 train-acc:0.731\n",
      "i:120 test-acc:0.8629 train-acc:0.753\n",
      "i:121 test-acc:0.8609 train-acc:0.743\n",
      "i:122 test-acc:0.8627 train-acc:0.752\n",
      "i:123 test-acc:0.8646 train-acc:0.76\n",
      "i:124 test-acc:0.8649 train-acc:0.766\n",
      "i:125 test-acc:0.8659 train-acc:0.752\n",
      "i:126 test-acc:0.868 train-acc:0.756\n",
      "i:127 test-acc:0.8648 train-acc:0.767\n",
      "i:128 test-acc:0.8662 train-acc:0.747\n",
      "i:129 test-acc:0.8669 train-acc:0.753\n",
      "i:130 test-acc:0.8694 train-acc:0.753\n",
      "i:131 test-acc:0.8692 train-acc:0.76\n",
      "i:132 test-acc:0.8658 train-acc:0.756\n",
      "i:133 test-acc:0.8666 train-acc:0.769\n",
      "i:134 test-acc:0.8692 train-acc:0.77\n",
      "i:135 test-acc:0.8681 train-acc:0.757\n",
      "i:136 test-acc:0.8705 train-acc:0.77\n",
      "i:137 test-acc:0.8706 train-acc:0.77\n",
      "i:138 test-acc:0.8684 train-acc:0.768\n",
      "i:139 test-acc:0.8664 train-acc:0.774\n",
      "i:140 test-acc:0.8666 train-acc:0.756\n",
      "i:141 test-acc:0.8705 train-acc:0.783\n",
      "i:142 test-acc:0.87 train-acc:0.775\n",
      "i:143 test-acc:0.8729 train-acc:0.769\n",
      "i:144 test-acc:0.8725 train-acc:0.776\n",
      "i:145 test-acc:0.8721 train-acc:0.772\n",
      "i:146 test-acc:0.8718 train-acc:0.765\n",
      "i:147 test-acc:0.8746 train-acc:0.777\n",
      "i:148 test-acc:0.8746 train-acc:0.77\n",
      "i:149 test-acc:0.8734 train-acc:0.778\n",
      "i:150 test-acc:0.873 train-acc:0.785\n",
      "i:151 test-acc:0.8732 train-acc:0.76\n",
      "i:152 test-acc:0.8727 train-acc:0.779\n",
      "i:153 test-acc:0.8754 train-acc:0.772\n",
      "i:154 test-acc:0.8729 train-acc:0.773\n",
      "i:155 test-acc:0.8758 train-acc:0.784\n",
      "i:156 test-acc:0.8732 train-acc:0.774\n",
      "i:157 test-acc:0.8743 train-acc:0.782\n",
      "i:158 test-acc:0.8762 train-acc:0.772\n",
      "i:159 test-acc:0.8755 train-acc:0.79\n",
      "i:160 test-acc:0.8751 train-acc:0.774\n",
      "i:161 test-acc:0.8749 train-acc:0.782\n",
      "i:162 test-acc:0.8744 train-acc:0.78\n",
      "i:163 test-acc:0.8765 train-acc:0.782\n",
      "i:164 test-acc:0.8738 train-acc:0.796\n",
      "i:165 test-acc:0.8753 train-acc:0.798\n",
      "i:166 test-acc:0.8767 train-acc:0.794\n",
      "i:167 test-acc:0.8746 train-acc:0.784\n",
      "i:168 test-acc:0.8769 train-acc:0.796\n",
      "i:169 test-acc:0.8758 train-acc:0.789\n",
      "i:170 test-acc:0.8764 train-acc:0.79\n",
      "i:171 test-acc:0.873 train-acc:0.791\n",
      "i:172 test-acc:0.8765 train-acc:0.797\n",
      "i:173 test-acc:0.8772 train-acc:0.789\n",
      "i:174 test-acc:0.8778 train-acc:0.781\n",
      "i:175 test-acc:0.8758 train-acc:0.799\n",
      "i:176 test-acc:0.8773 train-acc:0.785\n",
      "i:177 test-acc:0.8766 train-acc:0.796\n",
      "i:178 test-acc:0.8782 train-acc:0.803\n",
      "i:179 test-acc:0.8789 train-acc:0.794\n",
      "i:180 test-acc:0.8778 train-acc:0.794\n",
      "i:181 test-acc:0.8778 train-acc:0.8\n",
      "i:182 test-acc:0.8785 train-acc:0.791\n",
      "i:183 test-acc:0.8777 train-acc:0.787\n",
      "i:184 test-acc:0.8769 train-acc:0.781\n",
      "i:185 test-acc:0.8765 train-acc:0.786\n",
      "i:186 test-acc:0.8765 train-acc:0.793\n",
      "i:187 test-acc:0.8785 train-acc:0.796\n",
      "i:188 test-acc:0.879 train-acc:0.789\n",
      "i:189 test-acc:0.8763 train-acc:0.79\n",
      "i:190 test-acc:0.8774 train-acc:0.787\n",
      "i:191 test-acc:0.8766 train-acc:0.782\n",
      "i:192 test-acc:0.8803 train-acc:0.798\n",
      "i:193 test-acc:0.8781 train-acc:0.789\n",
      "i:194 test-acc:0.8795 train-acc:0.785\n",
      "i:195 test-acc:0.8791 train-acc:0.807\n",
      "i:196 test-acc:0.8778 train-acc:0.796\n",
      "i:197 test-acc:0.8783 train-acc:0.801\n",
      "i:198 test-acc:0.8778 train-acc:0.81\n",
      "i:199 test-acc:0.8771 train-acc:0.784\n",
      "i:200 test-acc:0.8776 train-acc:0.792\n",
      "i:201 test-acc:0.8784 train-acc:0.794\n",
      "i:202 test-acc:0.8787 train-acc:0.795\n",
      "i:203 test-acc:0.8803 train-acc:0.781\n",
      "i:204 test-acc:0.8798 train-acc:0.804\n",
      "i:205 test-acc:0.8779 train-acc:0.779\n",
      "i:206 test-acc:0.8788 train-acc:0.792\n",
      "i:207 test-acc:0.8764 train-acc:0.793\n",
      "i:208 test-acc:0.8792 train-acc:0.792\n",
      "i:209 test-acc:0.8798 train-acc:0.803\n",
      "i:210 test-acc:0.8788 train-acc:0.804\n",
      "i:211 test-acc:0.8793 train-acc:0.797\n",
      "i:212 test-acc:0.8764 train-acc:0.791\n",
      "i:213 test-acc:0.8801 train-acc:0.801\n",
      "i:214 test-acc:0.8814 train-acc:0.799\n",
      "i:215 test-acc:0.8806 train-acc:0.79\n",
      "i:216 test-acc:0.8799 train-acc:0.8\n",
      "i:217 test-acc:0.8803 train-acc:0.802\n",
      "i:218 test-acc:0.8782 train-acc:0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:219 test-acc:0.8818 train-acc:0.797\n",
      "i:220 test-acc:0.8793 train-acc:0.799\n",
      "i:221 test-acc:0.8789 train-acc:0.815\n",
      "i:222 test-acc:0.8791 train-acc:0.816\n",
      "i:223 test-acc:0.8793 train-acc:0.809\n",
      "i:224 test-acc:0.8814 train-acc:0.795\n",
      "i:225 test-acc:0.8798 train-acc:0.799\n",
      "i:226 test-acc:0.8805 train-acc:0.806\n",
      "i:227 test-acc:0.88 train-acc:0.808\n",
      "i:228 test-acc:0.8782 train-acc:0.801\n",
      "i:229 test-acc:0.8802 train-acc:0.814\n",
      "i:230 test-acc:0.8807 train-acc:0.8\n",
      "i:231 test-acc:0.8809 train-acc:0.798\n",
      "i:232 test-acc:0.8805 train-acc:0.82\n",
      "i:233 test-acc:0.8795 train-acc:0.794\n",
      "i:234 test-acc:0.8807 train-acc:0.806\n",
      "i:235 test-acc:0.8806 train-acc:0.808\n",
      "i:236 test-acc:0.8787 train-acc:0.802\n",
      "i:237 test-acc:0.8796 train-acc:0.81\n",
      "i:238 test-acc:0.8766 train-acc:0.805\n",
      "i:239 test-acc:0.8781 train-acc:0.792\n",
      "i:240 test-acc:0.8787 train-acc:0.809\n",
      "i:241 test-acc:0.8762 train-acc:0.802\n",
      "i:242 test-acc:0.8775 train-acc:0.811\n",
      "i:243 test-acc:0.8804 train-acc:0.814\n",
      "i:244 test-acc:0.8794 train-acc:0.804\n",
      "i:245 test-acc:0.8788 train-acc:0.801\n",
      "i:246 test-acc:0.8777 train-acc:0.795\n",
      "i:247 test-acc:0.8785 train-acc:0.808\n",
      "i:248 test-acc:0.8788 train-acc:0.803\n",
      "i:249 test-acc:0.8773 train-acc:0.813\n",
      "i:250 test-acc:0.8786 train-acc:0.808\n",
      "i:251 test-acc:0.8787 train-acc:0.803\n",
      "i:252 test-acc:0.8789 train-acc:0.812\n",
      "i:253 test-acc:0.8792 train-acc:0.804\n",
      "i:254 test-acc:0.8779 train-acc:0.815\n",
      "i:255 test-acc:0.8796 train-acc:0.811\n",
      "i:256 test-acc:0.8798 train-acc:0.806\n",
      "i:257 test-acc:0.88 train-acc:0.803\n",
      "i:258 test-acc:0.8776 train-acc:0.795\n",
      "i:259 test-acc:0.8798 train-acc:0.803\n",
      "i:260 test-acc:0.8799 train-acc:0.805\n",
      "i:261 test-acc:0.8789 train-acc:0.807\n",
      "i:262 test-acc:0.8784 train-acc:0.804\n",
      "i:263 test-acc:0.8792 train-acc:0.806\n",
      "i:264 test-acc:0.8777 train-acc:0.796\n",
      "i:265 test-acc:0.8785 train-acc:0.821\n",
      "i:266 test-acc:0.8794 train-acc:0.81\n",
      "i:267 test-acc:0.8783 train-acc:0.816\n",
      "i:268 test-acc:0.8777 train-acc:0.812\n",
      "i:269 test-acc:0.8791 train-acc:0.812\n",
      "i:270 test-acc:0.878 train-acc:0.813\n",
      "i:271 test-acc:0.8784 train-acc:0.82\n",
      "i:272 test-acc:0.8792 train-acc:0.821\n",
      "i:273 test-acc:0.8781 train-acc:0.823\n",
      "i:274 test-acc:0.8788 train-acc:0.816\n",
      "i:275 test-acc:0.8793 train-acc:0.82\n",
      "i:276 test-acc:0.8781 train-acc:0.829\n",
      "i:277 test-acc:0.8795 train-acc:0.809\n",
      "i:278 test-acc:0.875 train-acc:0.806\n",
      "i:279 test-acc:0.8795 train-acc:0.813\n",
      "i:280 test-acc:0.88 train-acc:0.816\n",
      "i:281 test-acc:0.8796 train-acc:0.819\n",
      "i:282 test-acc:0.8802 train-acc:0.809\n",
      "i:283 test-acc:0.8804 train-acc:0.811\n",
      "i:284 test-acc:0.8779 train-acc:0.808\n",
      "i:285 test-acc:0.8816 train-acc:0.82\n",
      "i:286 test-acc:0.8792 train-acc:0.822\n",
      "i:287 test-acc:0.8791 train-acc:0.817\n",
      "i:288 test-acc:0.8769 train-acc:0.814\n",
      "i:289 test-acc:0.8785 train-acc:0.807\n",
      "i:290 test-acc:0.8778 train-acc:0.817\n",
      "i:291 test-acc:0.8794 train-acc:0.82\n",
      "i:292 test-acc:0.8804 train-acc:0.824\n",
      "i:293 test-acc:0.8779 train-acc:0.812\n",
      "i:294 test-acc:0.8784 train-acc:0.816\n",
      "i:295 test-acc:0.877 train-acc:0.817\n",
      "i:296 test-acc:0.8767 train-acc:0.826\n",
      "i:297 test-acc:0.8774 train-acc:0.816\n",
      "i:298 test-acc:0.8774 train-acc:0.804\n",
      "i:299 test-acc:0.8774 train-acc:0.814"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    \n",
    "    # Iterate over mini-batches\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end=((i * batch_size), ((i+1)*batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28) # (128, 28, 28)\n",
    "        \n",
    "         # Slide a 3x3 kernel over the 28x28 image and extract subregions (sect)\n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start+kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "        \n",
    "        # Flatten and concatenate all subregions to form the input for the next layer\n",
    "        expanded_input = np.concatenate(sects,axis=1)\n",
    "        es = expanded_input.shape #(128, 625, 3, 3)\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1) #(80000, 9)\n",
    "        \n",
    "        # Project the flattened input through the kernels to form the inputs for the next layer\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0], -1))\n",
    "        \n",
    "        # Implement dropout: Randomly zero out some of the units in layer_1 to prevent overfitting\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        \n",
    "        # Calculate outputs of final layer by projecting layer_1 through the second set of weights, followed by softmax activation\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        # Increment correct count if the highest-scoring output unit matches the target label\n",
    "        for k in range(batch_size):\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == np.argmax(labelset))\n",
    "            correct_cnt += _inc\n",
    "        \n",
    "        # backprop\n",
    "        # Compute deltas for output and hidden layers and update weights    \n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= alpha * k_update\n",
    "        \n",
    "    test_correct_cnt = 0\n",
    "    \n",
    "    for i in range(len(test_images)):\n",
    "        \n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "        layer_0.shape\n",
    "        \n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start+kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "        \n",
    "        expanded_input = np.concatenate(sects, axis=1)\n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "        \n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "    if (j % 1 == 0):\n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"i:\" + str(j) + \\\n",
    "                         \" test-acc:\" + str(test_correct_cnt/float(len(test_images))) + \\\n",
    "                         \" train-acc:\" + str(correct_cnt/float(len(images))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5f749",
   "metadata": {},
   "outputs": [],
   "source": [
    " # reshape       \n",
    "    #     pretend each individual subregion is its own image.\n",
    "        #     if we have batch size of 8 images, and 100 subregions per image, \n",
    "        #         then we pretend it was a batch of 800 smaller images.\n",
    "        #     forward propogating them through a linear layer with one output neuron is the same as predicting\n",
    "        #     that linear layer over every subregion in every batch.\n",
    "        #     if we instead forward propagated using a linear layer with n output neurons, it will generate\n",
    "        #     the outputs that are the same as predicting n linear layers (kernels) in every pos of the image.\n",
    "        #     1 output neuron is both simpler and faster. \n",
    "        #     1 output neuron is akin to predicting a single propertty/feature in each subregion\n",
    "        #     n output neurons is like predicting n features/values for each subregion.\n",
    "        #     using more neurons in o0utput layer allows model to potentially map more complex features or combinations\n",
    "        #     of features from the subregions. however, it comes at cost of requirng more data for robust training and\n",
    "        #     being more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when a nn needs to use the same idea in multiple places, endeavor to use the same weights in both places."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grokking",
   "language": "python",
   "name": "grokking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
